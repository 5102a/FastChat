version: '3.9'

services:
  fastchat-controller:
    # build:
    #   context: .
    #   dockerfile: Dockerfile
    image: lc5102a/fastchat:v0.1
    ports:
      - '21001:21001'
    entrypoint: [python3.9, -m, fastchat.serve.controller, --host, 0.0.0.0, --port, '21001']
  fastchat-model-worker:
    # build:
    #   context: .
    #   dockerfile: Dockerfile
    volumes:
      - ${LLM}\THUDM:/THUDM
      - huggingface:/root/.cache/huggingface
    environment:
      FASTCHAT_CONTROLLER_URL: http://fastchat-controller:21001
    image: lc5102a/fastchat:v0.1
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    entrypoint: [python3.9, -m, fastchat.serve.model_worker, --model-path, THUDM/chatglm2-6b, --worker-address, 'http://fastchat-model-worker:21002', --controller-address, 'http://fastchat-controller:21001', --host, 0.0.0.0, --port, '21002']
  # fastchat-api-server:
  #   # build:
  #   #   context: .
  #   #   dockerfile: Dockerfile
  #   environment:
  #     FASTCHAT_CONTROLLER_URL: http://fastchat-controller:21001
  #   image: 5102a/fastchat:0.1
  #   ports:
  #     - "8000:8000"
  #   entrypoint: ["python3.9", "-m", "fastchat.serve.openai_api_server", "--controller-address", "http://fastchat-controller:21001", "--host", "0.0.0.0", "--port", "8000"]
volumes:
  huggingface:
